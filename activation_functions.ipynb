{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bk9frofMTyd1"
   },
   "source": [
    "This notebook will compare a neural network that uses activation functions vs one that does not use activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions are a critical component in neural networks, enabling them to model complex patterns and learn non-linear relationships. They determine the output of a neuron, effectively deciding whether the neuron should be activated or not based on its input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Sigmoid Function**:\n",
    "   - **Formula**: \n",
    "     \\[\n",
    "     \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "     \\]\n",
    "   - **Range**: \\( (0, 1) \\)\n",
    "   - **Characteristics**:\n",
    "     - Converts any input to a value between 0 and 1.\n",
    "     - Historically used in early neural networks, especially for binary classification problems.\n",
    "   - **Drawbacks**:\n",
    "     - **Vanishing Gradient Problem**: Gradients become very small for extreme values of \\( x \\), leading to slow learning or even stopping the learning process.\n",
    "     - **Output Not Zero-Centered**: Can cause gradients to have inconsistent signs, which may affect convergence speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example input tensor\n",
    "x = torch.tensor([-1.0, 0.0, 1.0])\n",
    "\n",
    "# Using the Sigmoid activation function\n",
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(x)\n",
    "\n",
    "print(\"Sigmoid Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Tanh (Hyperbolic Tangent) Function**:\n",
    "   - **Formula**: \n",
    "     \\[\n",
    "     \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "     \\]\n",
    "   - **Range**: \\( (-1, 1) \\)\n",
    "   - **Characteristics**:\n",
    "     - Similar to the sigmoid function but with outputs ranging between -1 and 1.\n",
    "     - Zero-centered output, which helps in better convergence during training.\n",
    "   - **Drawbacks**:\n",
    "     - Still suffers from the vanishing gradient problem, though to a lesser extent than the sigmoid function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Tanh activation function\n",
    "tanh = nn.Tanh()\n",
    "output = tanh(x)\n",
    "\n",
    "print(\"Tanh Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. **ReLU (Rectified Linear Unit) Function**:\n",
    "   - **Formula**: \n",
    "     \\[\n",
    "     \\text{ReLU}(x) = \\max(0, x)\n",
    "     \\]\n",
    "   - **Range**: \\( [0, \\infty) \\)\n",
    "   - **Characteristics**:\n",
    "     - Computationally efficient and simple to implement.\n",
    "     - Introduces non-linearity, allowing the model to learn complex patterns.\n",
    "     - Widely used in modern neural networks, especially in deep learning.\n",
    "   - **Drawbacks**:\n",
    "     - **Dying ReLU Problem**: Neurons can \"die\" if they constantly output zero (e.g., for negative inputs), making them inactive and not contributing to learning.\n",
    "     - Can cause some neurons to become inactive if the learning rate is too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the ReLU activation function\n",
    "relu = nn.ReLU()\n",
    "output = relu(x)\n",
    "\n",
    "print(\"ReLU Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Leaky ReLU**:\n",
    "   - **Formula**: \n",
    "     \\[\n",
    "     \\text{Leaky ReLU}(x) = \\begin{cases} \n",
    "     x & \\text{if } x > 0 \\\\\n",
    "     \\alpha x & \\text{if } x \\leq 0 \n",
    "     \\end{cases}\n",
    "     \\]\n",
    "     where \\( \\alpha \\) is a small positive constant (e.g., 0.01).\n",
    "   - **Range**: \\( (-\\infty, \\infty) \\)\n",
    "   - **Characteristics**:\n",
    "     - Addresses the dying ReLU problem by allowing a small, non-zero gradient when the input is negative.\n",
    "     - Helps keep neurons active during training.\n",
    "   - **Variants**:\n",
    "     - **Parametric ReLU (PReLU)**: Similar to Leaky ReLU, but \\( \\alpha \\) is a learnable parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Leaky ReLU activation function\n",
    "leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "output = leaky_relu(x)\n",
    "\n",
    "print(\"Leaky ReLU Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **ELU (Exponential Linear Unit)**:\n",
    "   - **Formula**: \n",
    "     \\[\n",
    "     \\text{ELU}(x) = \\begin{cases} \n",
    "     x & \\text{if } x > 0 \\\\\n",
    "     \\alpha (e^x - 1) & \\text{if } x \\leq 0 \n",
    "     \\end{cases}\n",
    "     \\]\n",
    "   - **Range**: \\( (-\\alpha, \\infty) \\) where \\( \\alpha \\) is a hyperparameter.\n",
    "   - **Characteristics**:\n",
    "     - ELU is designed to bring the average of the activations closer to zero, which speeds up learning.\n",
    "     - Helps reduce the bias shift effect and maintains non-linear properties.\n",
    "   - **Drawbacks**:\n",
    "     - More computationally expensive than ReLU due to the exponential operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the ELU activation function\n",
    "elu = nn.ELU(alpha=1.0)\n",
    "output = elu(x)\n",
    "\n",
    "print(\"ELU Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Swish**:\n",
    "   - **Formula**: \n",
    "     \\[\n",
    "     \\text{Swish}(x) = x \\cdot \\sigma(x)\n",
    "     \\]\n",
    "     where \\( \\sigma(x) \\) is the sigmoid function.\n",
    "   - **Range**: \\( (-\\infty, \\infty) \\)\n",
    "   - **Characteristics**:\n",
    "     - Smooth non-linearity that often outperforms ReLU in certain deep models.\n",
    "     - Allows small negative values, which can help in learning.\n",
    "   - **Drawbacks**:\n",
    "     - More complex than ReLU, leading to slightly higher computational cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Swish using PyTorch operations\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "output = swish(x)\n",
    "\n",
    "print(\"Swish Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7. **Softmax Function**:\n",
    "   - **Formula**: \n",
    "     \\[\n",
    "     \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "     \\]\n",
    "   - **Range**: \\( (0, 1) \\) for each element, with all outputs summing to 1.\n",
    "   - **Characteristics**:\n",
    "     - Used in the output layer of classification networks to represent probabilities over multiple classes.\n",
    "   - **Drawbacks**:\n",
    "     - Can be computationally intensive for a large number of classes due to the exponentiation and normalization operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input tensor (logits for a classification task)\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "\n",
    "# Using the Softmax activation function\n",
    "softmax = nn.Softmax(dim=0)  # dim=0 because we want to apply softmax along the first axis\n",
    "output = softmax(x)\n",
    "\n",
    "print(\"Softmax Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Choosing an Activation Function:\n",
    "- **ReLU and its variants (Leaky ReLU, PReLU, ELU)** are typically the default choice for hidden layers due to their simplicity and effectiveness in practice.\n",
    "- **Sigmoid** and **Tanh** are generally less favored for hidden layers due to their tendency to cause vanishing gradients but might still be used in certain network architectures or specific layers.\n",
    "- **Softmax** is specifically used in multi-class classification problems at the output layer to produce a probability distribution over classes.\n",
    "- **Swish** is a more recent innovation and might be chosen in some deep learning applications where its performance benefits are observed.\n",
    "\n",
    "The choice of activation function can significantly impact the training dynamics and final performance of a neural network, so itâ€™s important to consider the characteristics and drawbacks of each function when designing a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTdzDwH7TNq_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-adxKIgIXqPN"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 100 # Number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbqXuV5bTTnN"
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=INPUT_DIM,\n",
    "    n_informative=INPUT_DIM,\n",
    "    n_redundant=0,\n",
    "    random_state=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CzCe5KauTUIi",
    "outputId": "6871f657-a46b-4efe-c2a3-4527165d1168"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 100), (5000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUMc-SY6Tbso"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xg6BWpkVT8VC",
    "outputId": "390fb3cc-a755-41ad-8e17-39fba1d40470"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 100), (1000, 100))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYb-cPzrTd_u"
   },
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2xR_W4kbhyo"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, use_activation):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(INPUT_DIM, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        self.use_activation = use_activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) if self.use_activation else self.fc1(x)\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIBerxSGTiru"
   },
   "outputs": [],
   "source": [
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(model):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor.view(-1, 1))\n",
    "        if epoch % 5 == 0:\n",
    "          print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        pred = model(X_test_tensor)\n",
    "        predictions = (pred > 0.5).float().numpy()\n",
    "        accuracy = accuracy_score(y_test_tensor, predictions)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SfMpJeChZjfd",
    "outputId": "6c5828ed-1f18-45df-fec7-adb62edee509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9071, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7036, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5720, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.4863, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.4303, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3936, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3688, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3513, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3386, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3291, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3219, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3162, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3118, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3082, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3053, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3030, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3011, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.2996, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.2984, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.2974, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model without activation functions\n",
    "model_without_activation = NeuralNetwork(use_activation=False)\n",
    "accuracy_without_activation = train_and_evaluate(model_without_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ulxda_SVTuAL",
    "outputId": "314684c2-c32a-41ce-cba9-d54a7e8ef29a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0072, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.8534, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.7328, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6412, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5724, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5201, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.4793, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.4465, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.4188, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3950, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3742, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3557, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3388, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3231, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.3085, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.2947, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.2817, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.2695, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.2578, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.2467, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model with activation functions\n",
    "model_with_activation = NeuralNetwork(use_activation=True)\n",
    "accuracy_with_activation = train_and_evaluate(model_with_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vP8g7LHvTMAZ",
    "outputId": "fdd3e867-90cf-42ef-fa65-445c4166fe37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without Activation Functions: 0.864\n",
      "Accuracy with Activation Functions: 0.897\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy without Activation Functions:\", accuracy_without_activation)\n",
    "print(\"Accuracy with Activation Functions:\", accuracy_with_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QopRic-6YvKb"
   },
   "source": [
    "Note there may be situations where the adding of activation function decreases performance. This could be because adding activations causes overfitting. And maybe adding dropout could be useful."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
