{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMU-REPEJdLn"
   },
   "source": [
    "This notebook compares the performance of a neural network that uses Batch Normalization to one that does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization is a technique used in deep learning to stabilize and speed up the training process of neural networks by normalizing the inputs of each layer. It was introduced by Sergey Ioffe and Christian Szegedy in 2015 and has since become a standard practice in training deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Normalization**:\n",
    "   - Batch normalization normalizes the output of a previous activation layer by adjusting and scaling the activations.\n",
    "   - It subtracts the batch mean and divides by the batch standard deviation for each feature, ensuring that the output has a mean of zero and a variance of one.\n",
    "\n",
    "2. **Learnable Parameters**:\n",
    "   - After normalization, batch normalization introduces two learnable parameters: \n",
    "     - **Gamma (γ)**: A scaling factor.\n",
    "     - **Beta (β)**: A shifting factor.\n",
    "   - These parameters allow the network to undo the normalization if it is optimal for the learning task, effectively giving the model the flexibility to adjust the scale and mean of the normalized output.\n",
    "\n",
    "3. **Where It's Applied**:\n",
    "   - Batch normalization is typically applied after the activation function of a layer, though it can also be applied before the activation.\n",
    "\n",
    "4. **Effect on Training**:\n",
    "   - **Stabilizes Learning**: By normalizing the inputs to each layer, batch normalization helps in stabilizing the learning process, which can allow for higher learning rates.\n",
    "   - **Reduces Internal Covariate Shift**: It mitigates the problem where the distribution of each layer's inputs changes during training, known as internal covariate shift. This stabilization helps the model converge faster.\n",
    "   - **Regularization Effect**: Batch normalization has a slight regularization effect, as the noise introduced by estimating statistics (mean and variance) from mini-batches acts as a form of regularization, reducing the need for dropout in some cases.\n",
    "\n",
    "5. **Training and Inference Mode**:\n",
    "   - During training, batch normalization uses the statistics (mean and variance) of the current batch.\n",
    "   - During inference, it uses the running average of the batch statistics collected during training to maintain consistency.\n",
    "\n",
    "6. **Mathematical Formulation**:\n",
    "   Given an input \\( x \\) to a layer, the batch normalization process involves:\n",
    "   1. **Compute the mean and variance**:\n",
    "      \\[\n",
    "      \\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i, \\quad \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\n",
    "      \\]\n",
    "   2. **Normalize the input**:\n",
    "      \\[\n",
    "      \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "      \\]\n",
    "      where \\( \\epsilon \\) is a small constant added for numerical stability.\n",
    "   3. **Scale and shift**:\n",
    "      \\[\n",
    "      y_i = \\gamma \\hat{x}_i + \\beta\n",
    "      \\]\n",
    "      where \\( \\gamma \\) and \\( \\beta \\) are learnable parameters.\n",
    "\n",
    "### Benefits of Batch Normalization:\n",
    "- **Faster Training**: By allowing higher learning rates, batch normalization can significantly speed up the training process.\n",
    "- **Reduced Sensitivity to Initialization**: Models become less sensitive to the choice of initial weights, making it easier to train deep networks.\n",
    "- **Improved Generalization**: The regularization effect can lead to better generalization on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-KJdzTRSfCX4",
    "outputId": "b6280834-f277-498a-faf0-bce76282a227"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training without Batch Normalization:\n",
      "[1,   100] loss: 1.166\n",
      "[1,   200] loss: 0.446\n",
      "[1,   300] loss: 0.361\n",
      "[1,   400] loss: 0.323\n",
      "[1,   500] loss: 0.288\n",
      "[1,   600] loss: 0.267\n",
      "[1,   700] loss: 0.231\n",
      "[1,   800] loss: 0.246\n",
      "[1,   900] loss: 0.210\n",
      "[2,   100] loss: 0.200\n",
      "[2,   200] loss: 0.174\n",
      "[2,   300] loss: 0.171\n",
      "[2,   400] loss: 0.157\n",
      "[2,   500] loss: 0.155\n",
      "[2,   600] loss: 0.145\n",
      "[2,   700] loss: 0.135\n",
      "[2,   800] loss: 0.146\n",
      "[2,   900] loss: 0.145\n",
      "[3,   100] loss: 0.132\n",
      "[3,   200] loss: 0.127\n",
      "[3,   300] loss: 0.109\n",
      "[3,   400] loss: 0.117\n",
      "[3,   500] loss: 0.110\n",
      "[3,   600] loss: 0.106\n",
      "[3,   700] loss: 0.099\n",
      "[3,   800] loss: 0.094\n",
      "[3,   900] loss: 0.095\n",
      "\n",
      "Training with Batch Normalization:\n",
      "[1,   100] loss: 0.464\n",
      "[1,   200] loss: 0.253\n",
      "[1,   300] loss: 0.219\n",
      "[1,   400] loss: 0.172\n",
      "[1,   500] loss: 0.183\n",
      "[1,   600] loss: 0.152\n",
      "[1,   700] loss: 0.161\n",
      "[1,   800] loss: 0.149\n",
      "[1,   900] loss: 0.143\n",
      "[2,   100] loss: 0.118\n",
      "[2,   200] loss: 0.106\n",
      "[2,   300] loss: 0.098\n",
      "[2,   400] loss: 0.102\n",
      "[2,   500] loss: 0.101\n",
      "[2,   600] loss: 0.105\n",
      "[2,   700] loss: 0.105\n",
      "[2,   800] loss: 0.102\n",
      "[2,   900] loss: 0.096\n",
      "[3,   100] loss: 0.076\n",
      "[3,   200] loss: 0.071\n",
      "[3,   300] loss: 0.076\n",
      "[3,   400] loss: 0.078\n",
      "[3,   500] loss: 0.070\n",
      "[3,   600] loss: 0.084\n",
      "[3,   700] loss: 0.083\n",
      "[3,   800] loss: 0.074\n",
      "[3,   900] loss: 0.084\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Transformations for the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Loading MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define a simple neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, use_batch_norm=False):\n",
    "        super(Net, self).__init__()\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "        # Batch Normalization layers\n",
    "        if use_batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(512)\n",
    "            self.bn2 = nn.BatchNorm1d(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "\n",
    "        # Apply batch normalization if specified\n",
    "        if self.use_batch_norm:\n",
    "            x = self.bn1(x)\n",
    "\n",
    "        x = torch.relu(self.fc2(x))\n",
    "\n",
    "        # Apply batch normalization if specified\n",
    "        if self.use_batch_norm:\n",
    "            x = self.bn2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, criterion, optimizer, epochs=3):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "\n",
    "# Training without batch normalization\n",
    "model_no_bn = Net(use_batch_norm=False)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_no_bn.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "print(\"Training without Batch Normalization:\")\n",
    "train_model(model_no_bn, criterion, optimizer)\n",
    "\n",
    "# Training with batch normalization\n",
    "model_with_bn = Net(use_batch_norm=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_with_bn.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "print(\"\\nTraining with Batch Normalization:\")\n",
    "train_model(model_with_bn, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AtuQiXpSgP1W",
    "outputId": "2b82f660-c700-45c4-e308-aa229bfe06e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy without Batch Normalization: 96.77%\n",
      "Accuracy with Batch Normalization: 97.33%\n"
     ]
    }
   ],
   "source": [
    "# Transformations for the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Loading MNIST test dataset\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def get_accuracy(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Print accuracy for both models\n",
    "print(\"\\nAccuracy without Batch Normalization: {:.2%}\".format(get_accuracy(model_no_bn, testloader)))\n",
    "print(\"Accuracy with Batch Normalization: {:.2%}\".format(get_accuracy(model_with_bn, testloader)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B755UWceicci"
   },
   "source": [
    "Slightly faster convergence with better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mmu9GPC5gQPJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
